{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_img_path = \"../data/student_images\"\n",
    "instructor_img_path = \"../data/instructor_images\"\n",
    "\n",
    "prompt_path = \"../prompts/csv_promt.txt\"\n",
    "csv_output = \"output.csv\"\n",
    "\n",
    "student_response = process_form(prompt_path, student_img_path)\n",
    "instructor_response = process_form(prompt_path, instructor_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_csv(student_response, \"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_instruct_answer_to_csv(instructor_response, \"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_question(question, student_answer, instructor_answer):\n",
    "    gpt3_model = TextModel(model_name=\"gpt-3.5-turbo-0125\")\n",
    "    router_prompt = (f\"Given the question: '{question}', evaluate if the student's answer: '{student_answer}' \"\n",
    "              f\"is 100% correct against the instructor's answer: '{instructor_answer}'. \"\n",
    "              f\"Return '(correct:1)' if the student's answer is fully correct, or '(correct:0)'\\\n",
    "                  if the answer is incorrect or if you are unsure.\")\n",
    "    first_evaluation = gpt3_model.complete(prompt=router_prompt, role=\"user\")\n",
    "    eval_result = 1 if \"(correct:1)\" in first_evaluation else 0\n",
    "    if eval_result: return \"Correct answer\"\n",
    "\n",
    "    gpt4_model = TextModel(model_name=\"gpt-4-1106-preview\")\n",
    "    cot_prompt = (\n",
    "        f\"Given the question: '{question}', evaluate step by step and succinctly answer why the student's answer: '{student_answer}' \"\n",
    "              f\"is partially or completely wrong against the instructor's answer: '{instructor_answer}'. \"\n",
    "    )\n",
    "    second_evaluation = gpt4_model.complete(cot_prompt)\n",
    "    return second_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.TextModel at 0x117742170>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextModel(model_name=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "def extract_questions(pdf_path):\n",
    "    questions = []\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.strip().endswith('?') or 'Solve:' in line or 'Evaluate:' in line or 'Simplify:' in line or 'Use the' in line or 'Find an' in line or 'Graph the' in line or 'Expand:' in line:\n",
    "                    questions.append(line.strip())\n",
    "    return questions\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = 'Questions.pdf'  # Change this to the actual path of your PDF file\n",
    "questions = extract_questions(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    client = OpenAI()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Assuming 'questions' is your list of question strings\n",
    "questions_df = pd.DataFrame(questions, columns=['question'])\n",
    "\n",
    "# Apply the embedding function to each question\n",
    "questions_df['embedding'] = questions_df['question'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_embeddings(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Assuming embeddings are stored as string representations of lists\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=','))\n",
    "    return df\n",
    "\n",
    "# Function to manually compute cosine similarity\n",
    "def cosine_similarity_manual(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Function to compute cosine similarity and return top k matches\n",
    "def top_k_matched_questions(query, k=5):\n",
    "    # Path to the CSV file\n",
    "    csv_path = 'embedded_questions.csv'\n",
    "    questions_df = read_embeddings(csv_path)\n",
    "    \n",
    "    # Get embedding for the query\n",
    "    query_embedding = get_embedding(query)  # Assuming get_embedding is defined elsewhere and set up properly\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = np.array([cosine_similarity_manual(query_embedding, np.array(embedding)) for embedding in questions_df['embedding']])\n",
    "    \n",
    "    # Get top k indices\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "    \n",
    "    # Return the top k matched questions\n",
    "    return questions_df.iloc[top_k_indices]['question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7. Simplify: 8y -2-3(y-4)',\n",
       " '6. Use the distributive property to simplify. -3(x-10)+x',\n",
       " '3. Simplify: 6 – 2* 2 + (2^5)',\n",
       " '13. Solve: 3(x−5)  < x−8',\n",
       " '2. Simplify: 3+4*5 -6']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_matched_questions(\"Simplify the expression 2x^2 - 8 / x - 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
